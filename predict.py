# predict_fixed.py
import mlflow
import mlflow.pyfunc
import pandas as pd
import requests
import json
import numpy as np
from common import CONFIG, build_data

mlflow.set_tracking_uri(CONFIG['mlflow']['tracking_uri'])
print(f"MLflow Tracking URI: {mlflow.get_tracking_uri()}")

def predict_with_loaded_model():
    """ì§ì ‘ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ì˜ˆì¸¡"""
    model_name = CONFIG['project']['model_name']
    model_stage = CONFIG['mlflow']['model_stage']
    model_uri = f"models:/{model_name}/{model_stage}"
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model = mlflow.pyfunc.load_model(model_uri)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        X_train, X_test, y_train, y_test = build_data()
        
        # ì˜ˆì¸¡
        predictions = model.predict(X_test.head(5))
        
        print("Direct Model Predictions:")
        for i, pred in enumerate(predictions):
            actual = y_test.iloc[i].values[0]
            print(f"Sample {i+1}: Predicted={pred:.2f}, Actual={actual:.2f}")
        
        return predictions
    except Exception as e:
        print(f"Error in direct prediction: {e}")
        return None

def predict_with_api_v2():
    """MLflow 2.0+ í˜¸í™˜ API ì˜ˆì¸¡"""
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    X_train, X_test, y_train, y_test = build_data()
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì‚¬ìš©
    sample_data = X_test.iloc[0].to_dict()
    feature_names = list(X_test.columns)
    sample_values = [X_test.iloc[0].tolist()]
    
    # MLflow 2.0+ í˜¸í™˜ í˜•ì‹ë“¤ (inputs í˜•ì‹ ì œê±° - ìŠ¤í‚¤ë§ˆ ë¬¸ì œë¡œ ì¸í•´)
    request_formats = [
        {
            "name": "dataframe_split",
            "data": {
                "dataframe_split": {
                    "columns": feature_names,
                    "data": sample_values
                }
            }
        },
        {
            "name": "instances", 
            "data": {
                "instances": [sample_data]
            }
        }
        # inputs í˜•ì‹ì€ ì œê±° - MLflow ëª¨ë¸ì´ feature ì´ë¦„ì„ ìš”êµ¬í•¨
    ]
    
    url = "http://localhost:8081/invocations"
    headers = {"Content-Type": "application/json"}
    
    successful_prediction = None
    
    for format_info in request_formats:
        try:
            print(f"\n=== Testing {format_info['name']} format ===")
            print(f"Request data preview: {format_info['name']} with {len(feature_names)} features")
            
            response = requests.post(url, json=format_info['data'], headers=headers, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                
                # ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹…
                print(f"Raw response type: {type(result)}")
                print(f"Raw response: {result}")
                
                # ê²°ê³¼ íŒŒì‹± (ë‹¤ì–‘í•œ í˜•íƒœ ì²˜ë¦¬)
                prediction = None
                
                if isinstance(result, list):
                    # [1.23] ë˜ëŠ” [[1.23]] í˜•íƒœ
                    if len(result) > 0:
                        if isinstance(result[0], (list, np.ndarray)):
                            prediction = result[0][0]  # [[1.23]] -> 1.23
                        else:
                            prediction = result[0]  # [1.23] -> 1.23
                elif isinstance(result, dict):
                    # {"prediction": 1.23} ë˜ëŠ” {"predictions": [1.23]} í˜•íƒœ
                    if 'prediction' in result:
                        prediction = result['prediction']
                    elif 'predictions' in result:
                        predictions = result['predictions']
                        if isinstance(predictions, list) and len(predictions) > 0:
                            prediction = predictions[0]
                    else:
                        # dict ìì²´ê°€ ì˜ˆì¸¡ ê²°ê³¼ì¼ ìˆ˜ë„ ìˆìŒ - ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                        values = list(result.values())
                        if values:
                            prediction = values[0]
                elif isinstance(result, (int, float)):
                    # ì§ì ‘ ìˆ«ì
                    prediction = result
                else:
                    print(f"Unexpected result type: {type(result)}")
                    print(f"Result content: {result}")
                    continue
                
                # ìµœì¢… ìˆ«ì ë³€í™˜
                if prediction is None:
                    print(f"Could not extract prediction from: {result}")
                    continue
                
                # ë¦¬ìŠ¤íŠ¸ë‚˜ ë°°ì—´ì´ë©´ ì²« ë²ˆì§¸ ìš”ì†Œ ì¶”ì¶œ
                if isinstance(prediction, (list, np.ndarray)):
                    if len(prediction) > 0:
                        prediction = prediction[0]
                    else:
                        print("Empty prediction array")
                        continue
                
                # ìµœì¢… float ë³€í™˜
                try:
                    prediction = float(prediction)
                except (ValueError, TypeError) as e:
                    print(f"Cannot convert prediction to float: {prediction}, error: {e}")
                    continue
                
                actual = float(y_test.iloc[0].values[0])
                
                print(f"âœ… {format_info['name']} format SUCCESS")
                print(f"Predicted: {prediction:.2f}")
                print(f"Actual: {actual:.2f}")
                print(f"Difference: {abs(prediction - actual):.2f}")
                
                successful_prediction = prediction
                break  # ì„±ê³µí•˜ë©´ ì¤‘ë‹¨
                
            else:
                print(f"âŒ {format_info['name']} format FAILED: {response.status_code}")
                error_msg = response.text[:200] + "..." if len(response.text) > 200 else response.text
                print(f"Error: {error_msg}")
                
        except requests.exceptions.ConnectionError:
            print("âŒ Cannot connect to API server")
            print("Make sure the server is running: python serve_model.py")
            return None
        except requests.exceptions.Timeout:
            print(f"âŒ {format_info['name']} format TIMEOUT")
        except ValueError as e:
            print(f"âŒ {format_info['name']} format VALUE ERROR: {e}")
        except Exception as e:
            print(f"âŒ {format_info['name']} format UNEXPECTED ERROR: {e}")
    
    return successful_prediction

def get_sample_request_format():
    """ì˜¬ë°”ë¥¸ ìš”ì²­ í˜•ì‹ ì˜ˆì‹œ ìƒì„±"""
    try:
        X_train, X_test, y_train, y_test = build_data()
        sample = X_test.iloc[0]
        
        formats = {
            "dataframe_split": {
                "dataframe_split": {
                    "columns": list(sample.index),
                    "data": [sample.tolist()]
                }
            },
            "instances": {
                "instances": [sample.to_dict()]
            }
            # inputs í˜•ì‹ ì œê±° - feature ì´ë¦„ì´ í•„ìš”í•œ ëª¨ë¸ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŒ
        }
        
        print("=== MLflow 2.0+ Compatible Request Formats ===")
        for name, format_data in formats.items():
            print(f"\n{name.upper()} format:")
            print(json.dumps(format_data, indent=2)[:300] + "..." if len(json.dumps(format_data, indent=2)) > 300 else json.dumps(format_data, indent=2))
        
        print(f"\nFeature names: {list(sample.index)}")
        print(f"Sample values: {sample.tolist()}")
        
        return formats
        
    except Exception as e:
        print(f"Error generating sample formats: {e}")
        return None

def test_api_connection():
    """API ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        response = requests.get("http://localhost:8081/health", timeout=5)
        if response.status_code == 200:
            print("âœ… API server is running")
            return True
        else:
            print(f"âŒ API server returned: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ API server is not accessible")
        return False
    except Exception as e:
        print(f"âŒ Connection test failed: {e}")
        return False

def compare_predictions():
    """ì§ì ‘ ëª¨ë¸ê³¼ API ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ"""
    print("="*60)
    print("ğŸ” PREDICTION COMPARISON")
    print("="*60)
    
    # 1. ì§ì ‘ ëª¨ë¸ ì˜ˆì¸¡
    print("\n1ï¸âƒ£ Direct Model Prediction:")
    direct_predictions = predict_with_loaded_model()
    
    if direct_predictions is None:
        print("âŒ Direct prediction failed")
        return
    
    # 2. API ì—°ê²° í™•ì¸
    print("\n2ï¸âƒ£ API Connection Test:")
    if not test_api_connection():
        print("âŒ API is not available. Make sure to run: python serve_model.py")
        return
    
    # 3. API ì˜ˆì¸¡
    print("\n3ï¸âƒ£ API Prediction:")
    api_prediction = predict_with_api_v2()
    
    # 4. ê²°ê³¼ ë¹„êµ
    if api_prediction is not None and direct_predictions is not None:
        direct_first = float(direct_predictions[0])
        api_first = float(api_prediction)
        
        print("\n" + "="*60)
        print("ğŸ“Š RESULTS SUMMARY")
        print("="*60)
        print(f"Direct Model Prediction: {direct_first:.4f}")
        print(f"API Prediction:          {api_first:.4f}")
        print(f"Difference:              {abs(direct_first - api_first):.6f}")
        
        if abs(direct_first - api_first) < 0.001:
            print("âœ… Predictions match! API is working correctly.")
        else:
            print("âš ï¸  Predictions differ. Check model versions or data preprocessing.")

if __name__ == "__main__":
    compare_predictions()